{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"oov-words\"></a>\n",
    "# Out of vocabulary words (OOV)\n",
    "<a name=\"vocabulary\"></a>\n",
    "### Vocabulary\n",
    "In the video about the out of vocabulary words, you saw that the first step in dealing with the unknown words is to decide which words belong to the vocabulary. \n",
    "\n",
    "In the code assignment, you will try the method based on minimum frequency - all words appearing in the training set with frequency >= minimum frequency are added to the vocabulary.\n",
    "\n",
    "Here is a code for the other method, where the target size of the vocabulary is known in advance and the vocabulary is filled with words based on their frequency in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary from M most frequent words\n",
    "# use Counter object from the collections library to find M most common words\n",
    "from collections import Counter\n",
    "\n",
    "# the target size of the vocabulary\n",
    "M = 3\n",
    "\n",
    "# pre-calculated word counts\n",
    "# Counter could be used to build this dictionary from the source corpus\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n",
    "\n",
    "# Counter.most_common(M) returns a list of the M most common words along with their counts\n",
    "vocabulary = Counter(word_counts).most_common(M)\n",
    "\n",
    "# remove the frequencies and leave just the words\n",
    "vocabulary = [w[0] for w in vocabulary]\n",
    "\n",
    "print(f\"The new vocabulary containing {M} most frequent words: {vocabulary}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the vocabulary is ready, you can use it to replace the OOV words with $<UNK>$ as you saw in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: ['am', 'i', 'learning']\n",
      "\n",
      "output sentence: ['<UNK>', '<UNK>', 'learning']\n"
     ]
    }
   ],
   "source": [
    "# test if words in the input sentences are in the vocabulary, if OOV, print <UNK>\n",
    "\n",
    "sentence = ['am', 'i', 'learning']\n",
    "output_sentence = []\n",
    "\n",
    "print(f\"input sentence: {sentence}\")\n",
    "print()\n",
    "\n",
    "for w in sentence:\n",
    "    # test if word w is in vocabulary\n",
    "    if w in vocabulary:                # vocabulary = ['happy', 'because', 'learning']\n",
    "        output_sentence.append(w)\n",
    "    else:\n",
    "        output_sentence.append('<UNK>')\n",
    "        \n",
    "print(f\"output sentence: {output_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building the vocabulary in the code assignment, you will need to know how to iterate through the word counts dictionary. \n",
    "\n",
    "Here is an example of a similar task showing how to go through all the word counts and print out only the words with the frequency equal to `f`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "# iterate through all word counts and print words with given frequency f\n",
    "\n",
    "f = 3   # Set the target frequency to filter words by\n",
    "\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n",
    "\n",
    "for word, freq in word_counts.items():  # Iterate through all word-frequency pairs in the word_counts dictionary\n",
    "    if freq == f:                       # If the frequency matches (3), print the word\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the videos, if there are many $<UNK>$ replacements in your train and test set, you may get a very low perplexity even though the model itself wouldn't be very helpful. \n",
    "    \n",
    "Here is a sample code showing this unwanted effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for the training set: 1.2599210498948732\n",
      "Perplexity for the training set with <UNK>: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define the training set containing words seen during training\n",
    "training_set = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "\n",
    "# Define a modified training set where rare or unseen words are replaced with <UNK>\n",
    "training_set_unk = ['i', 'am', '<UNK>', '<UNK>', 'i', 'am', '<UNK>', '<UNK>']\n",
    "\n",
    "# Define the test set to evaluate the model\n",
    "test_set = ['i', 'am', 'learning']\n",
    "\n",
    "# Define a modified test set where rare or unseen words are replaced with <UNK>\n",
    "test_set_unk = ['i', 'am', '<UNK>']\n",
    "\n",
    "# Calculate the length of the test set\n",
    "M = len(test_set)\n",
    "\n",
    "# Initialize the probability for the test set to 1 (will be updated by multiplying bigram probabilities)\n",
    "probability = 1\n",
    "\n",
    "# Initialize the probability for the test set with <UNK> to 1\n",
    "probability_unk = 1\n",
    "\n",
    "# Pre-calculated bigram probabilities for pairs of words in the training set\n",
    "bigram_probabilities = {\n",
    "    ('i', 'am'): 1.0,          # P(am|i) = 1.0\n",
    "    ('am', 'happy'): 0.5,      # P(happy|am) = 0.5\n",
    "    ('happy', 'because'): 1.0, # P(because|happy) = 1.0\n",
    "    ('because', 'i'): 1.0,     # P(i|because) = 1.0\n",
    "    ('am', 'learning'): 0.5,   # P(learning|am) = 0.5\n",
    "    ('learning', '.'): 1.0     # P(.|learning) = 1.0\n",
    "}\n",
    "\n",
    "# Pre-calculated bigram probabilities for the test set with <UNK>\n",
    "bigram_probabilities_unk = {\n",
    "    ('i', 'am'): 1.0,           # P(am|i) = 1.0\n",
    "    ('am', '<UNK>'): 1.0,       # P(<UNK>|am) = 1.0\n",
    "    ('<UNK>', '<UNK>'): 0.5,    # P(<UNK>|<UNK>) = 0.5\n",
    "    ('<UNK>', 'i'): 0.25        # P(i|<UNK>) = 0.25\n",
    "}\n",
    "\n",
    "# Iterate through the test set to calculate its bigram probability\n",
    "for i in range(len(test_set) - 2 + 1):\n",
    "    \n",
    "    bigram = tuple(test_set[i: i + 2])  # Get the bigram (pair of consecutive words) from the test set \n",
    "    \n",
    "    probability = probability * bigram_probabilities[bigram]  # Multiply the current probability by the bigram probability\n",
    "    \n",
    "    bigram_unk = tuple(test_set_unk[i: i + 2])  # Get the bigram from the test set with <UNK>\n",
    "    \n",
    "    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]  # Multiply the current probability by \n",
    "                                                                              # the bigram probability with <UNK>\n",
    "        \n",
    "# Calculate perplexity for the original test set\n",
    "perplexity = probability ** (-1 / M)\n",
    "\n",
    "# Calculate perplexity for the test set with <UNK>\n",
    "perplexity_unk = probability_unk ** (-1 / M)\n",
    "\n",
    "# Print the calculated perplexities\n",
    "print(f\"Perplexity for the training set: {perplexity}\")\n",
    "print(f\"Perplexity for the training set with <UNK>: {perplexity_unk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"smoothing\"></a>\n",
    "### Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add-k smoothing was described as a method for smoothing of the probabilities for previously unseen n-grams. \n",
    "\n",
    "Here is an example code that shows how to implement add-k smoothing but also highlights a disadvantage of this method. The downside is that n-grams not previously seen in the training dataset get too high probability. \n",
    "\n",
    "In the code output bellow you'll see that a phrase that is in the training set gets the same probability as an unknown phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability_known_trigram: 0.2\n",
      "Probability_unknown_trigram: 0.2\n"
     ]
    }
   ],
   "source": [
    "def add_k_smoothing_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
    "    \"\"\"\n",
    "    Calculates the probability of an n-gram with add-k smoothing.\n",
    "    \n",
    "    Args:\n",
    "        k: Smoothing parameter, usually a small positive integer.\n",
    "        vocabulary_size: Total number of unique words in the vocabulary.\n",
    "        n_gram_count: The count of the specific n-gram (e.g., trigram) in the corpus.\n",
    "        n_gram_prefix_count: The count of the n-gram's prefix (e.g., the bigram for a trigram) in the corpus.\n",
    "        \n",
    "    Returns:\n",
    "        The smoothed probability of the n-gram.\n",
    "    \"\"\"\n",
    "    # Calculate the numerator by adding the smoothing factor k to the n-gram count\n",
    "    numerator = n_gram_count + k\n",
    "    \n",
    "    # Calculate the denominator by adding k multiplied by the vocabulary size to the prefix count\n",
    "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
    "    \n",
    "    # Return the smoothed probability\n",
    "    return numerator / denominator\n",
    "\n",
    "# Example n-gram counts from the corpus\n",
    "trigram_probabilities = {('i', 'am', 'happy') : 2}  # Count of the trigram \"i am happy\"\n",
    "bigram_probabilities = {( 'i', 'am') : 10}          # Count of the bigram \"i am\"\n",
    "\n",
    "# Define the size of the vocabulary\n",
    "vocabulary_size = 5\n",
    "\n",
    "# Define the smoothing parameter k\n",
    "k = 1\n",
    "\n",
    "# Calculate the smoothed probability for a known trigram \"i am happy\"\n",
    "probability_known_trigram = add_k_smoothing_probability(\n",
    "    k, \n",
    "    vocabulary_size, \n",
    "    trigram_probabilities[('i', 'am', 'happy')], \n",
    "    bigram_probabilities[('i', 'am')]\n",
    ")\n",
    "\n",
    "# Calculate the smoothed probability for an unknown trigram (not seen in the corpus)\n",
    "probability_unknown_trigram = add_k_smoothing_probability(k, vocabulary_size, 0, 0)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Probability_known_trigram: {probability_known_trigram}\")\n",
    "print(f\"Probability_unknown_trigram: {probability_unknown_trigram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"backoff\"></a>\n",
    "### Back-off\n",
    "Back-off is a model generalization method that leverages information from lower order n-grams in case information about the high order n-grams is missing. For example, if the probability of an trigram is missing, use bigram information and so on.\n",
    "\n",
    "Here you can see an example of a simple back-off technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n",
      "\n",
      "Probability for trigram ('are', 'you', 'happy') not found\n",
      "Probability for bigram ('you', 'happy') not found\n",
      "Probability for unigram happy found\n",
      "\n",
      "Probability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n"
     ]
    }
   ],
   "source": [
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = ('are', 'you', 'happy')\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1: 3]\n",
    "unigram = trigram[2]\n",
    "print(f\"Besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")  #<===== 1st print\n",
    "\n",
    "# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\n",
    "lambda_factor = 0.4\n",
    "probability_hat_trigram = 0\n",
    "\n",
    "# search for first non-zero probability starting with trigram\n",
    "# to generalize this for any order of n-gram hierarchy, \n",
    "# you could loop through the probability dictionaries instead of if/else cascade\n",
    "if trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n",
    "    print(f\"Probability for trigram {trigram} not found\")       #<===== 2nd print\n",
    "    \n",
    "    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n",
    "        print(f\"Probability for bigram {bigram} not found\")     #<===== 3rd print\n",
    "        \n",
    "        if unigram in unigram_probabilities:\n",
    "            print(f\"Probability for unigram {unigram} found\\n\") #<===== 4th print\n",
    "            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n",
    "        else:\n",
    "            probability_hat_trigram = 0\n",
    "    else:\n",
    "        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\n",
    "else:\n",
    "    probability_hat_trigram = trigram_probabilities[trigram]\n",
    "\n",
    "print(f\"Probability for trigram {trigram} estimated as {probability_hat_trigram}\")  #<===== 5th print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"interpolation\"></a>\n",
    "### Interpolation\n",
    "The other method for using probabilities of lower order n-grams is the interpolation. In this case, you use weighted probabilities of n-grams of all orders every time, not just when high order information is missing. \n",
    "\n",
    "For example, you always combine trigram, bigram and unigram probability. You can see how this in the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides the trigram ('i', 'am', 'happy'), we also use bigram ('am', 'happy') and unigram (happy)\n",
      "\n",
      "Estimated probability of the input trigram ('i', 'am', 'happy') is 0.185\n"
     ]
    }
   ],
   "source": [
    "# Pre-calculated probabilities for different types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0.15}  # Probability of the trigram \"i am happy\"\n",
    "bigram_probabilities = {('am', 'happy'): 0.3}         # Probability of the bigram \"am happy\"\n",
    "unigram_probabilities = {'happy': 0.4}                # Probability of the unigram \"happy\"\n",
    "\n",
    "# The weights are determined from optimization on a validation set\n",
    "lambda_1 = 0.8    # Weight for the trigram probability\n",
    "lambda_2 = 0.15   # Weight for the bigram probability\n",
    "lambda_3 = 0.05   # Weight for the unigram probability\n",
    "\n",
    "# The input trigram we need to estimate the probability for\n",
    "trigram = ('i', 'am', 'happy')\n",
    "\n",
    "# Extract the last bigram and unigram from the input trigram\n",
    "bigram = trigram[1: 3]    # \"am happy\"\n",
    "unigram = trigram[2]      # \"happy\"\n",
    "print(f\"Besides the trigram {trigram}, we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
    "\n",
    "# Calculate the estimated probability of the trigram using a linear interpolation of the n-grams\n",
    "# In production code, you would need to check if the probability n-gram dictionary contains the n-gram\n",
    "\n",
    "# Apply the weights to each n-gram probability and sum them to get the interpolated probability\n",
    "probability_hat_trigram = (\n",
    "    lambda_1 * trigram_probabilities[trigram] +  # Weighted trigram probability\n",
    "    lambda_2 * bigram_probabilities[bigram] +    # Weighted bigram probability\n",
    "    lambda_3 * unigram_probabilities[unigram]    # Weighted unigram probability\n",
    ")\n",
    "\n",
    "# Print the final estimated probability of the input trigram\n",
    "print(f\"Estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n",
      "\n",
      "Estimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n"
     ]
    }
   ],
   "source": [
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0.15}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# the weights come from optimization on a validation set\n",
    "lambda_1 = 0.8\n",
    "lambda_2 = 0.15\n",
    "lambda_3 = 0.05\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = ('i', 'am', 'happy')\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1: 3]\n",
    "unigram = trigram[2]\n",
    "print(f\"Besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
    "\n",
    "# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\n",
    "probability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n",
    "+ lambda_2 * bigram_probabilities[bigram]\n",
    "+ lambda_3 * unigram_probabilities[unigram]\n",
    "\n",
    "print(f\"Estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
