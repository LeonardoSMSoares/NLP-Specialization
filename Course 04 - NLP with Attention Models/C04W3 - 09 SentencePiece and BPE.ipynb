{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece and BPE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process text in neural network models it is first required to **encode** text as numbers with ids, since the tensor operations act on numbers. Finally, if the output of the network is to be words, it is required to **decode** the predicted tokens ids back to text.\n",
    "\n",
    "To encode text, the first decision that has to be made is to what level of granularity are we going to consider the text? Because ultimately, from these **tokens**, features are going to be created about them. Many different experiments have been carried out using *words*, *morphological units*, *phonemic units* or *characters* as tokens. For example, \n",
    "\n",
    "- Tokens are tricky. (raw text)\n",
    "- Tokens are tricky . ([words](https://arxiv.org/pdf/1301.3781))\n",
    "- Token s _ are _ trick _ y . ([morphemes](https://arxiv.org/pdf/1907.02423.pdf))\n",
    "- t oʊ k ə n z _ ɑː _ ˈt r ɪ k i. ([phonemes](https://www.aclweb.org/anthology/W18-5812.pdf), for STT)\n",
    "- T o k e n s _ a r e _ t r i c k y . ([character](https://www.aclweb.org/anthology/C18-1139/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how to identify these units, such as words, is largely determined by the language they come from. For example, in many European languages a space is used to separate words, while in some Asian languages there are no spaces between words. Compare English and Mandarin.\n",
    "\n",
    "- Tokens are tricky. (original sentence)\n",
    "- 标记很棘手 (Mandarin)\n",
    "- Biāojì hěn jíshǒu (pinyin)\n",
    "- 标记 很 棘手 (Mandarin with spaces)\n",
    "\n",
    "\n",
    "So, the ability to **tokenize**, i.e. split text into meaningful fundamental units, is not always straight-forward.\n",
    "\n",
    "Also, there are practical issues of how large our *vocabulary* of words, `vocab_size`, should be, considering memory limitations vs. coverage. A compromise may be need to be made between: \n",
    "* the finest-grained models employing characters which can be memory intensive and \n",
    "* more computationally efficient *subword* units such as [n-grams](https://arxiv.org/pdf/1712.09405) or larger units.\n",
    "\n",
    "In [SentencePiece](https://www.aclweb.org/anthology/D18-2012.pdf) unicode characters are grouped together using either a [unigram language model](https://www.aclweb.org/anthology/P18-1007.pdf) (used in this week's assignment) or [BPE](https://arxiv.org/pdf/1508.07909.pdf), **byte-pair encoding**. We will discuss BPE, since BERT and many of its variants use a modified version of BPE and its pseudocode is easy to implement and understand... hopefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece Preprocessing\n",
    "### NFKC Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, even using unicode to initially tokenize text can be ambiguous, e.g., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é = é : False\n"
     ]
    }
   ],
   "source": [
    "eaccent = '\\u00E9'\n",
    "e_accent = '\\u0065\\u0301'\n",
    "\n",
    "print(f'{eaccent} = {e_accent} : {eaccent == e_accent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece uses the Unicode standard normalization form, [NFKC](https://en.wikipedia.org/wiki/Unicode_equivalence), so this isn't an issue. Looking at the example from above but with normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é = é : True\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "norm_eaccent = normalize('NFKC', '\\u00E9')\n",
    "norm_e_accent = normalize('NFKC', '\\u0065\\u0301')\n",
    "\n",
    "print(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization has actually changed the unicode code point (unicode unique id) for one of these two characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hex_encoding(s):\n",
    "    \"\"\"Convert each character in the string `s` to its hexadecimal representation.\"\"\"\n",
    "    # For each character `c` in the string `s`, get its Unicode code point using `ord(c)` and \n",
    "    # convert it to a hexadecimal string using `hex()`\n",
    "    # Join these hexadecimal representations with a space separator\n",
    "    return ' '.join(hex(ord(c)) for c in s)\n",
    "\n",
    "def print_string_and_encoding(s):\n",
    "    \"\"\"Print the string `s` along with its hexadecimal encoding.\"\"\"\n",
    "    # Call `get_hex_encoding(s)` to get the hexadecimal representation of each character in the string `s`\n",
    "    # Print the original string `s` and its hexadecimal encoding\n",
    "    print(f'{s} : {get_hex_encoding(s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é : 0xe9\n",
      "é : 0x65 0x301\n",
      "é : 0xe9\n",
      "é : 0xe9\n"
     ]
    }
   ],
   "source": [
    "for s in [eaccent, e_accent, norm_eaccent, norm_e_accent]:\n",
    "    print_string_and_encoding(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This normalization has other side effects which may be considered useful such as converting curly quotes &ldquo; to \" their ASCII equivalent. (<sup>*</sup>Although we *now* lose directionality of the quote...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossless Tokenization\n",
    "\n",
    "SentencePiece also ensures that when you tokenize your data and detokenize your data the original position of white space is preserved. However, tabs and newlines are converted to spaces.\n",
    "\n",
    "To ensure this **lossless tokenization**, SentencePiece replaces white space with _ (U+2581). So that a simple join of the tokens by replacing underscores with spaces can restore the white space, even if there are consecutive symbols. But remember first to normalize and then replace spaces with _ (U+2581)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Tokenization is hard.'\n",
    "sn = normalize('NFKC', s)\n",
    "sn_ = sn.replace(' ', '\\u2581')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n"
     ]
    }
   ],
   "source": [
    "print(get_hex_encoding(s))\n",
    "print(get_hex_encoding(sn))\n",
    "print(get_hex_encoding(sn_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Algorithm\n",
    "\n",
    "After discussing the preprocessing that SentencePiece performs, you will get the data, preprocess it, and apply the BPE algorithm. You will see how this reproduces the tokenization produced by training SentencePiece on the example dataset (from this week's assignment).\n",
    "\n",
    "### Preparing our Data\n",
    "First, you get the Squad data and process it as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_json_examples_to_text(filepath):\n",
    "    example_jsons = list(map(ast.literal_eval, open(filepath))) # Read in the json from the example file\n",
    "    texts = [example_json['text'].decode('utf-8') for example_json in example_jsons] # Decode the byte sequences\n",
    "    text = '\\n\\n'.join(texts)       # Separate different articles by two newlines\n",
    "    text = normalize('NFKC', text)  # Normalize the text\n",
    "\n",
    "    with open('example.txt', 'w') as fw:\n",
    "        fw.write(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
      "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\n",
      "The cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n",
      "\n",
      "Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\n",
      "I've got a 500gb internal drive and a 240gb SSD.\n",
      "When trying to restore using di\n"
     ]
    }
   ],
   "source": [
    "text = convert_json_examples_to_text('./data/data.txt')\n",
    "print(text[:900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the algorithm the `vocab` variable is actually a frequency dictionary of the words. Those words have been prepended with an *underscore* to indicate that they are the beginning of a word. Finally, the characters have been delimited by spaces so that the BPE algorithm can group the most common characters together in the dictionary in a greedy fashion. You will see how that is done shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a Counter object for words in `text`, prefixing each word with the Unicode character '\\u2581'\n",
    "vocab = Counter(['\\u2581' + word for word in text.split()])\n",
    "\n",
    "# Convert each word in `vocab` to a space-separated format and rebuild the dictionary\n",
    "vocab = {' '.join([l for l in word]): freq for word, freq in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_vocab(vocab, end='\\n', limit=20):\n",
    "    \n",
    "    \"\"\"Show word frequencies in vocab up to the limit number of words\"\"\"  # Function docstring explaining its purpose\n",
    "\n",
    "    shown = 0                              # Initialize a counter for the number of words shown\n",
    "    \n",
    "    for word, freq in vocab.items():       # Iterate over each word and its frequency in the vocab dictionary\n",
    "        print(f'{word}: {freq}', end=end)  # Print the word and its frequency, ending with the specified end character\n",
    "        shown += 1                         # Increment the counter\n",
    "        if shown > limit:                  # If the number of words shown exceeds the limit, break the loop\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ B e g i n n e r s: 1\n",
      "▁ B B Q: 3\n",
      "▁ C l a s s: 2\n",
      "▁ T a k i n g: 1\n",
      "▁ P l a c e: 1\n",
      "▁ i n: 15\n",
      "▁ M i s s o u l a !: 1\n",
      "▁ D o: 1\n",
      "▁ y o u: 13\n",
      "▁ w a n t: 1\n",
      "▁ t o: 33\n",
      "▁ g e t: 2\n",
      "▁ b e t t e r: 2\n",
      "▁ a t: 1\n",
      "▁ m a k i n g: 2\n",
      "▁ d e l i c i o u s: 1\n",
      "▁ B B Q ?: 1\n",
      "▁ Y o u: 1\n",
      "▁ w i l l: 6\n",
      "▁ h a v e: 4\n",
      "▁ t h e: 31\n"
     ]
    }
   ],
   "source": [
    "show_vocab(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You check the size of the vocabulary (frequency dictionary) because this is the one hyperparameter that BPE depends on crucially on how far it breaks up a word into SentencePieces. It turns out that for your trained model on the small dataset that 60% of 455 merges of the most frequent characters need to be done to reproduce the upperlimit of a 32K `vocab_size` over the entire corpus of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 455\n",
      "Number of merges required to reproduce SentencePiece training on the whole corpus: 273\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of unique words: {len(vocab)}')\n",
    "\n",
    "print(f'Number of merges required to reproduce SentencePiece training on the whole corpus: {int(0.60*len(vocab))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Algorithm\n",
    "Directly from the BPE paper you have the following algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    Calculate frequency of adjacent pairs of symbols in the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        vocab (dict): Dictionary of word frequencies.\n",
    "        \n",
    "    Returns:\n",
    "        pairs (defaultdict): Frequency of each symbol pair.\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)   # Create a default dictionary to store the frequency of symbol pairs\n",
    "    for word, freq in vocab.items():       # Iterate over each word and its frequency in the vocabulary\n",
    "        symbols = word.split()             # Split the word into a list of symbols (usually characters or subwords)\n",
    "        for i in range(len(symbols) - 1):  # Loop through symbols to find adjacent pairs\n",
    "            pairs[symbols[i], symbols[i+1]] += freq  # Increment the count of the symbol pair by the word frequency\n",
    "    return pairs                                     # Return the dictionary of symbol pairs and their frequencies\n",
    "\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    Merge the most frequent pair of symbols in the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        pair (tuple): The most frequent symbol pair to be merged.\n",
    "        v_in (dict): The current vocabulary dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        v_out (dict): Updated vocabulary after merging the symbol pair.\n",
    "    \"\"\"\n",
    "    v_out = {}                                       # Create an empty dictionary for the updated vocabulary\n",
    "    bigram = re.escape(' '.join(pair))               # Join the pair into a bigram and escape special characters for regex\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')  # Compile a regex pattern to match the bigram as a whole word\n",
    "    for word in v_in:                                # Iterate over each word in the input vocabulary\n",
    "        w_out = p.sub(''.join(pair), word)           # Replace the bigram with the merged symbol in the word\n",
    "        v_out[w_out] = v_in[word]                    # Update the output vocabulary with the merged word\n",
    "    return v_out                                     # Return the updated vocabulary\n",
    "\n",
    "\n",
    "def get_sentence_piece_vocab(vocab, frac_merges=0.60):\n",
    "    \"\"\"\n",
    "    Generate a SentencePiece-like vocabulary by performing a specified fraction of merges.\n",
    "    \n",
    "    Args:\n",
    "        vocab (dict): Original vocabulary of word frequencies.\n",
    "        frac_merges (float): Fraction of merges to perform (default is 0.60).\n",
    "    \n",
    "    Returns:\n",
    "        sp_vocab (dict): Updated vocabulary after merges.\n",
    "    \"\"\"\n",
    "    sp_vocab = vocab.copy()                        # Make a copy of the input vocabulary to avoid modifying the original\n",
    "    num_merges = int(len(sp_vocab) * frac_merges)  # Calculate the number of merges to perform based on the fraction\n",
    "    \n",
    "    for i in range(num_merges):                    # Loop to perform the specified number of merges\n",
    "        pairs = get_stats(sp_vocab)                # Get the frequency of all symbol pairs in the current vocabulary\n",
    "        best = max(pairs, key=pairs.get)           # Identify the most frequent symbol pair\n",
    "        sp_vocab = merge_vocab(best, sp_vocab)     # Merge the most frequent pair in the vocabulary\n",
    "\n",
    "    return sp_vocab                                # Return the updated vocabulary after the merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what's going on first take a look at the third function `get_sentence_piece_vocab`. It takes in the current `vocab` word-frequency dictionary and the fraction, `frac_merges`, of the total `vocab_size` to merge characters in the words of the dictionary, `num_merges` times. Then for each *merge* operation it `get_stats` on how many of each pair of character sequences there are. It gets the most frequent *pair* of symbols as the `best` pair. Then it merges that pair of symbols (removes the space between them) in each word in the `vocab` that contains this `best` (= `pair`). Consequently, `merge_vocab` creates a new `vocab`, `v_out`. This process is repeated `num_merges` times and the result is the set of SentencePieces (keys of the final `sp_vocab`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Discussion of BPE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please feel free to skip the below if the above description was enough.\n",
    "\n",
    "In a little more detail you can see in `get_stats` you initially create a list of bigram (two character sequence) frequencies from the vocabulary. Later, this may include trigrams, quadgrams, etc. Note that the key of the `pairs` frequency dictionary is actually a 2-tuple, which is just shorthand notation for a pair.\n",
    "\n",
    "In `merge_vocab` you take in an individual `pair` (of character sequences, note this is the most frequency `best` pair) and the current `vocab` as `v_in`. You create a new `vocab`, `v_out`, from the old by joining together the characters in the pair (removing the space), if they are present in a word of the dictionary.\n",
    "\n",
    "[Warning](https://regex101.com/): the expression `(?<!\\S)` means that either a whitespace character follows before the `bigram` or there is nothing before the bigram (it is the beginning of the word), similarly for `(?!\\S)` for preceding whitespace or the end of the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁B e g in n ers: 1\n",
      "▁BBQ: 3\n",
      "▁Cl ass: 2\n",
      "▁T ak ing: 1\n",
      "▁P la ce: 1\n",
      "▁in: 15\n",
      "▁M is s ou la !: 1\n",
      "▁D o: 1\n",
      "▁you: 13\n",
      "▁w an t: 1\n",
      "▁to: 33\n",
      "▁g et: 2\n",
      "▁be t ter: 2\n",
      "▁a t: 1\n",
      "▁mak ing: 2\n",
      "▁d e l ic i ou s: 1\n",
      "▁BBQ ?: 1\n",
      "▁ Y ou: 1\n",
      "▁will: 6\n",
      "▁have: 4\n",
      "▁the: 31\n"
     ]
    }
   ],
   "source": [
    "sp_vocab = get_sentence_piece_vocab(vocab)\n",
    "show_vocab(sp_vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SentencePiece BPE Tokenizer on Example Data\n",
    "### Explore SentencePiece Model\n",
    "First, explore the SentencePiece model provided with this week's assignment. Remember you can always use Python's built in `help` command to see the documentation for any object or method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor(model_file='./data/sentencepiece.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out on the first sentence of the example text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = 'Beginners BBQ Class Taking Place in Missoula!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Beginn', 'ers', '▁BBQ', '▁Class', '▁', 'Taking', '▁Place', '▁in', '▁Miss', 'oul', 'a', '!']\n",
      "[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55]\n",
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Beginners\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "# Convert the input text `s0` into a list of subword tokens (pieces)\n",
    "print(sp.encode_as_pieces(s0))                    # Output: List of subword pieces (tokens)\n",
    "\n",
    "# Convert the input text `s0` into a list of corresponding token IDs\n",
    "print(sp.encode_as_ids(s0))                       # Output: List of token IDs corresponding to the subword pieces\n",
    "\n",
    "# decode: id => text\n",
    "# Decode a list of subword pieces back into the original text\n",
    "print(sp.decode_pieces(sp.encode_as_pieces(s0)))  # Output: Reconstructed text from subword pieces\n",
    "\n",
    "# Decode a list of token IDs back into the original text\n",
    "print(sp.decode_ids([12847, 277]))                # Output: Reconstructed text from the given token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how SentencePiece breaks the words into seemingly odd parts, but you have seen something similar with BPE. But how close was the model trained on the whole corpus of examples with a `vocab_size` of 32,000 instead of 455? Here you can also test what happens to white space, like '\\n'. \n",
    "\n",
    "But first note that SentencePiece encodes the SentencePieces, the tokens, and has reserved some of the ids as can be seen in this week's assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece for ID 15068: ▁BBQ\n",
      "ID for Sentence Piece ▁BBQ: 15068\n",
      "ID for unknown text __MUST_BE_UNKNOWN__: 2\n"
     ]
    }
   ],
   "source": [
    "uid = 15068  # Example token ID\n",
    "spiece = \"\\u2581BBQ\"  # Example subword piece (prefixed with '\\u2581', a special symbol used in SentencePiece)\n",
    "unknown = \"__MUST_BE_UNKNOWN__\"  # Example of a text that is not in the vocabulary (unknown token)\n",
    "\n",
    "# id <=> piece conversion\n",
    "# Convert a token ID to its corresponding subword piece\n",
    "print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')         # Output: Subword piece corresponding to the given ID\n",
    "\n",
    "# Convert a subword piece to its corresponding token ID\n",
    "print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')  # Output: ID corresponding to the given subword piece\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the ID for UNK)\n",
    "# Convert an unknown text (not in the vocabulary) to its corresponding token ID, which is typically 0 by default\n",
    "print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')  # Output: ID for unknown token (usually 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of sentence id: -1\n",
      "Pad id: 0\n",
      "End of sentence id: 1\n",
      "Unknown id: 2\n",
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Print the ID for the beginning of the sentence token (BOS)\n",
    "print(f'Beginning of sentence id: {sp.bos_id()}')  # Output: ID used to represent the beginning of a sentence\n",
    "\n",
    "# Print the ID for the padding token (PAD)\n",
    "print(f'Pad id: {sp.pad_id()}')      # Output: ID used for padding, typically used to align sequences to the same length\n",
    "\n",
    "# Print the ID for the end of the sentence token (EOS)\n",
    "print(f'End of sentence id: {sp.eos_id()}')        # Output: ID used to represent the end of a sentence\n",
    "\n",
    "# Print the ID for the unknown token (UNK)\n",
    "print(f'Unknown id: {sp.unk_id()}')  # Output: ID used to represent tokens that are not in the vocabulary\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print(f'Vocab size: {sp.vocab_size()}')            # Output: Total number of tokens (pieces) in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check what are the ids for the first part and last part of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Id\tSentP\tControl?\n",
      "------------------------\n",
      "0\t<pad>\tTrue\n",
      "1\t</s>\tTrue\n",
      "2\t<unk>\tFalse\n",
      "3\t▁\tFalse\n",
      "4\tX\tFalse\n",
      "5\t.\tFalse\n",
      "6\t,\tFalse\n",
      "7\ts\tFalse\n",
      "8\t▁the\tFalse\n",
      "9\ta\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Print the header for the table displaying ID, SentencePiece, and whether it is a control symbol\n",
    "print('\\nId\\tSentP\\tControl?')  # Print column headers for ID, SentencePiece, and Control status\n",
    "print('------------------------')  # Print a separator line for readability\n",
    "\n",
    "# <unk>, <s>, </s> are defined by default in SentencePiece. Their ids are (0, 1, 2).\n",
    "# <s> (beginning of sentence) and </s> (end of sentence) are defined as 'control' symbols.\n",
    "\n",
    "# Iterate through the first 10 IDs to display their corresponding SentencePiece and control status\n",
    "for uid in range(10):  \n",
    "    print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')  # Output: ID, corresponding piece, \n",
    "                                                                   # and control status (True/False)\n",
    "\n",
    "# Uncomment the following code block to iterate over the last 10 IDs in the vocabulary and display their corresponding \n",
    "# SentencePiece and control status\n",
    "\n",
    "# for uid in range(sp.vocab_size()-10, sp.vocab_size()):  \n",
    "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')  # Output: ID, corresponding piece, and control status (True/False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SentencePiece BPE model with our example.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train your own BPE model directly from the SentencePiece library and compare it to the results of the implemention of the algorithm from the BPE paper itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['▁B', 'e', 'ginn', 'ers', '▁BBQ', '▁Cl', 'ass', '▁T', 'ak', 'ing', '▁P', 'la', 'ce', '▁in', '▁M', 'is', 's', 'ou', 'la', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: example.txt\n",
      "  input_format: \n",
      "  model_prefix: example_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 450\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: example.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 26 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=4533\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9559% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=73\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999559\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 26 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 26\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 455\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=99 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=20 all=732 active=658 piece=▁w\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=40 all=937 active=863 piece=ch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=60 all=1014 active=940 piece=▁u\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=80 all=1110 active=1036 piece=me\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=100 all=1166 active=1092 piece=la\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=120 all=1217 active=1042 piece=SD\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=140 all=1272 active=1097 piece=▁bu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=160 all=1288 active=1113 piece=▁site\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=180 all=1315 active=1140 piece=ter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=200 all=1330 active=1155 piece=asure\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=220 all=1339 active=1008 piece=ge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=240 all=1371 active=1040 piece=▁sh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=260 all=1384 active=1053 piece=▁cost\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=280 all=1391 active=1060 piece=de\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=300 all=1405 active=1074 piece=000\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=320 all=1427 active=1021 piece=▁GB\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=340 all=1438 active=1032 piece=last\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=360 all=1441 active=1035 piece=▁let\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: example_bpe.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: example_bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "# Train a SentencePiece model using BPE (Byte-Pair Encoding) on the input file 'example.txt'\n",
    "# '--input=example.txt': Specifies the input file containing text data for training\n",
    "# '--model_prefix=example_bpe': Sets the prefix for the output model files (example_bpe.model and example_bpe.vocab)\n",
    "# '--vocab_size=450': Defines the size of the vocabulary to be created\n",
    "# '--model_type=bpe': Specifies the model type to use, which is BPE (Byte-Pair Encoding) in this case\n",
    "spm.SentencePieceTrainer.train('--input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe')\n",
    "\n",
    "# Initialize a SentencePieceProcessor object to use the trained BPE model\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "\n",
    "# Load the trained BPE model from the file 'example_bpe.model'\n",
    "sp_bpe.load('example_bpe.model')\n",
    "\n",
    "# Print a header indicating that the following output is for the BPE encoding\n",
    "print('*** BPE ***')\n",
    "\n",
    "# Encode the input string `s0` into a list of subword tokens (pieces) using the BPE model\n",
    "print(sp_bpe.encode_as_pieces(s0))  # Output: List of subword pieces (tokens) obtained using the BPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁B e g in n ers: 1, ▁BBQ: 3, ▁Cl ass: 2, ▁T ak ing: 1, ▁P la ce: 1, ▁in: 15, ▁M is s ou la !: 1, ▁D o: 1, ▁you: 13, ▁w an t: 1, ▁to: 33, ▁g et: 2, ▁be t ter: 2, ▁a t: 1, ▁mak ing: 2, ▁d e l ic i ou s: 1, ▁BBQ ?: 1, ▁ Y ou: 1, ▁will: 6, ▁have: 4, ▁the: 31, "
     ]
    }
   ],
   "source": [
    "# Display the vocabulary stored in `sp_vocab` with word frequencies, using a comma as the separator between entries.\n",
    "# The `show_vocab` function takes a dictionary (`sp_vocab`) and prints up to a specified limit of word-frequency pairs.\n",
    "# `end=', '` specifies that the output should use a comma followed by a space to separate each entry.\n",
    "\n",
    "show_vocab(sp_vocab, end=', ')  # Output: Vocabulary displayed with a comma and space separating each word-frequency pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of BPE's code from the paper matches up pretty well with the library itself! The differences are probably accounted for by the `vocab_size`. There is also another technical difference in that in the SentencePiece implementation of BPE a priority queue is used to more efficiently keep track of the *best pairs*. Actually, there is a priority queue in the Python standard library called `heapq` if you would like to give that a try below! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally try to implement BPE using a priority queue below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "def heapsort(iterable):\n",
    "    \"\"\"Sort an iterable using the heapsort algorithm.\"\"\"\n",
    "    \n",
    "    h = []  # Initialize an empty list to use as a heap\n",
    "    \n",
    "    # Iterate over all values in the input iterable\n",
    "    for value in iterable:\n",
    "        heappush(h, value)                      # Push each value onto the heap to maintain the heap property\n",
    "\n",
    "    # Pop all elements from the heap and return them in sorted order\n",
    "    return [heappop(h) for i in range(len(h))]  # Extract the smallest elements one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 3, 3, 4, 4]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 4, 3, 1, 3, 2, 1, 4, 2]  # Initialize the list to be sorted\n",
    "\n",
    "# Sort the list `a` using the heapsort function\n",
    "sorted_a = heapsort(a)\n",
    "\n",
    "print(sorted_a)  # Output: [1, 1, 1, 2, 2, 3, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more extensive example consider looking at the [SentencePiece repo](https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb). The last few sections of this code were repurposed from that tutorial. Thanks for your participation! Next stop BERT and T5!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
